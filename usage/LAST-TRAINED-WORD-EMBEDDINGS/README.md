### The Corpus.
```
feeling very tired lately
persistent cough that won't go away
stomach hurts after eating
get headaches often
noticed my weight has been fluctuating a lot
trouble sleeping at night
short of breath after walking for a few minutes
joints are aching
nauseous every morning
experiencing dizziness
throat feels scratchy and dry
fever and chills
feeling anxious for no reason
```
#### _Corpus Description and Analysis_.
List of short, descriptive phrases that resemble symptoms or personal health-related observations.
1. **Short Sentences and Phrases**: A single concise sentence or phrase. This brevity results in `fewer co-occurrence` relationships between words `compared to longer texts`. 
2. **Repetition of Topics**: The corpus revolves around health-related terms, symptoms, and sensations (e.g., "tired," "cough," "nauseous"), creating a narrowly focused vocabulary.
3. **Natural Language**: The sentences are written in simple, colloquial English, making them closer to real-world data one might encounter in personal notes or symptom descriptions.
4. **Sparse Contextual Relationships**: Due to the fragmented nature of the phrases, some words might appear in isolation or without sufficient context to build rich co-occurrence patterns.

#### _Usefulness for Testing and Training_.
. **_Advantages_**:
1. **Controlled Vocabulary**: The small and domain-specific vocabulary makes it easier to manually inspect and validate the quality of embeddings generated by the model.
2. **Word Similarities**: Words that are semantically or contextually related (e.g., "tired" and "sleeping" or "cough" and "throat") can serve as a benchmark for evaluating whether the Skip-gram model captures meaningful relationships.
3. **Edge Case Handling**: Testing on such a small dataset helps identify whether the implementation can handle sparse data and still learn useful embeddings.

. **_Limitations_**:
1. **Limited Diversity**: The corpus lacks diversity in topics and sentence structure, which could limit the model's ability to generalize.
2. **Insufficient Context**: The brevity of phrases may lead to fewer meaningful co-occurrence pairs, reducing the richness of learned embeddings.
3. **Small Dataset Size**: The small size of the corpus may result in overfitting, where the model learns patterns specific to this dataset rather than generalizable word relationships.

. **_Recommendation_**:
While this corpus is useful for debugging and initial testing of the implementation, a larger and more diverse dataset is necessary to thoroughly validate the model's performance. Consider augmenting this corpus with additional text from other domains to test the robustness of your Skip-gram implementation and ensure it generalizes well to different types of language data.

### The Context-Target Token Representation.
```
NONE  NONE  NONE  NONE  [ feeling ] very  tired  lately  persistent
NONE  NONE  NONE  feeling  [ very ] tired  lately  persistent  cough
NONE  NONE  feeling  very  [ tired ] lately  persistent  cough  that
NONE  feeling  very  tired  [ lately ] persistent  cough  that  won't
feeling  very  tired  lately  [ persistent ] cough  that  won't  go
very  tired  lately  persistent  [ cough ] that  won't  go  away
tired  lately  persistent  cough  [ that ] won't  go  away  stomach
lately  persistent  cough  that  [ won't ] go  away  stomach  hurts
persistent  cough  that  won't  [ go ] away  stomach  hurts  after
cough  that  won't  go  [ away ] stomach  hurts  after  eating
that  won't  go  away  [ stomach ] hurts  after  eating  get
won't  go  away  stomach  [ hurts ] after  eating  get  headaches
go  away  stomach  hurts  [ after ] eating  get  headaches  often
away  stomach  hurts  after  [ eating ] get  headaches  often  noticed
stomach  hurts  after  eating  [ get ] headaches  often  noticed  my
hurts  after  eating  get  [ headaches ] often  noticed  my  weight
after  eating  get  headaches  [ often ] noticed  my  weight  has
eating  get  headaches  often  [ noticed ] my  weight  has  been
get  headaches  often  noticed  [ my ] weight  has  been  fluctuating
headaches  often  noticed  my  [ weight ] has  been  fluctuating  a
often  noticed  my  weight  [ has ] been  fluctuating  a  lot
noticed  my  weight  has  [ been ] fluctuating  a  lot  trouble
my  weight  has  been  [ fluctuating ] a  lot  trouble  sleeping
weight  has  been  fluctuating  [ a ] lot  trouble  sleeping  at
has  been  fluctuating  a  [ lot ] trouble  sleeping  at  night
been  fluctuating  a  lot  [ trouble ] sleeping  at  night  short
fluctuating  a  lot  trouble  [ sleeping ] at  night  short  of
a  lot  trouble  sleeping  [ at ] night  short  of  breath
lot  trouble  sleeping  at  [ night ] short  of  breath  after
trouble  sleeping  at  night  [ short ] of  breath  after  walking
sleeping  at  night  short  [ of ] breath  after  walking  for
at  night  short  of  [ breath ] after  walking  for  a
night  short  of  breath  [ after ] walking  for  a  few
short  of  breath  after  [ walking ] for  a  few  minutes
of  breath  after  walking  [ for ] a  few  minutes  joints
breath  after  walking  for  [ a ] few  minutes  joints  are
after  walking  for  a  [ few ] minutes  joints  are  aching
walking  for  a  few  [ minutes ] joints  are  aching  nauseous
for  a  few  minutes  [ joints ] are  aching  nauseous  every
a  few  minutes  joints  [ are ] aching  nauseous  every  morning
few  minutes  joints  are  [ aching ] nauseous  every  morning  experiencing
minutes  joints  are  aching  [ nauseous ] every  morning  experiencing  dizziness
joints  are  aching  nauseous  [ every ] morning  experiencing  dizziness  throat
are  aching  nauseous  every  [ morning ] experiencing  dizziness  throat  feels
aching  nauseous  every  morning  [ experiencing ] dizziness  throat  feels  scratchy
nauseous  every  morning  experiencing  [ dizziness ] throat  feels  scratchy  and
every  morning  experiencing  dizziness  [ throat ] feels  scratchy  and  dry
morning  experiencing  dizziness  throat  [ feels ] scratchy  and  dry  fever
experiencing  dizziness  throat  feels  [ scratchy ] and  dry  fever  and
dizziness  throat  feels  scratchy  [ and ] dry  fever  and  chills
throat  feels  scratchy  and  [ dry ] fever  and  chills  feeling
feels  scratchy  and  dry  [ fever ] and  chills  feeling  anxious
scratchy  and  dry  fever  [ and ] chills  feeling  anxious  for
and  dry  fever  and  [ chills ] feeling  anxious  for  no
dry  fever  and  chills  [ feeling ] anxious  for  no  reason
fever  and  chills  feeling  [ anxious ] for  no  reason  NONE
and  chills  feeling  anxious  [ for ] no  reason  NONE NONE
chills  feeling  anxious  for  [ no ] reason  NONE NONE NONE
feeling  anxious  for  no  [ reason ] NONE NONE NONE NONE
```

#### _Context-Target Token Representation Analysis_.
The representation provided by the model showcases the use of a context window to generate training pairs for the Skip-gram model. Here's a detailed analysis of this representation.

**_Structure and Window Size_**

- Each target word (enclosed in square brackets, e.g., [ feeling ]) is paired with its surrounding context words.
- The context window size appears to be `8` (four tokens on each side of the target word). For instance, in the line:
NONE NONE NONE NONE [ feeling ] very tired lately persistent, four NONE tokens are added to the beginning to handle edge cases where fewer context tokens are available.

**_Observations on the Window Size_**.
1. **Balanced Coverage**: A window size of 4 ensures a decent balance between capturing immediate local context (close words) and some slightly distant relationships. This is critical for learning meaningful word embeddings, especially for small datasets.
2. **Handling Edge Cases with Padding**: The use of `NONE` tokens at the start and end of the sequence ensures the context window remains consistent even for words near the beginning or end of the corpus. These tokens involving `NONE` are not used during training and we do that to not add noise thus yhese pairs do not dilute the quality of embeddings for words closer to the boundaries.

**_Strengths of the Representation_**.
1. **Consistent Context Generation**: The model maintains a fixed context size, which is essential for uniform training.
2. **Sufficient Contextual Information**: A window of size 4 provides enough tokens to establish relationships between words in short sentences like those in the corpus.
3. **Edge Padding**: The inclusion of NONE tokens avoids data wastage when the context window cannot be fully populated.

**_Potential Limitations_**.
1. **Sparse Relationships**: Since the corpus consists of short, fragmented phrases, some target words (e.g., "reason" or "very") may lack meaningful co-occurrence relationships, especially with a small vocabulary.
2. **Limited Window Scope**: While a window size of 4 is reasonable for short phrases, it might miss long-distance dependencies, such as relationships across phrases or sentences.

**_Recommendations_**.
1. **Optimize Window Size**: Depending on the corpus's size and linguistic structure, experimenting with slightly larger windows (e.g., `5–6`) might capture richer co-occurrence patterns, especially for datasets with more varied sentence structures.
2. **Exclude NONE Tokens from Training**: `You could filter out context-target pairs involving NONE` during preprocessing to reduce noise and improve the focus on meaningful word relationships.
3. **Expand Dataset**: To better evaluate the implementation, consider using a larger and more diverse dataset, where relationships across broader contexts can be captured.

**_Conclusion_**.
The current context-target representation is well-suited for initial testing of the Skip-gram implementation. The use of a context window size of 4 is practical for this corpus but may benefit from adjustments as you scale to larger and more complex datasets.

### Observations from First Training Session.

```
.\skipy.exe corpus ./INPUT.txt lr 0.0001 epoch 10 rs 0.001 loop 0 verbose --output w1p.dat w2p.dat
Dimensions of W1 = 54 X 16
Dimensions of W2 = 16 X 54
Epoch# 1 of 10 epochs.
epoch_loss = (574.778), Average epoch_loss = 9.742
Epoch# 2 of 10 epochs.
epoch_loss = (572.737), Average epoch_loss = 9.70741
Epoch# 3 of 10 epochs.
epoch_loss = (571.101), Average epoch_loss = 9.67967
Epoch# 4 of 10 epochs.
epoch_loss = (569.511), Average epoch_loss = 9.65272
Epoch# 5 of 10 epochs.
epoch_loss = (567.662), Average epoch_loss = 9.62139
Epoch# 6 of 10 epochs.
epoch_loss = (565.967), Average epoch_loss = 9.59266
Epoch# 7 of 10 epochs.
epoch_loss = (564.356), Average epoch_loss = 9.56536
Epoch# 8 of 10 epochs.
epoch_loss = (562.7), Average epoch_loss = 9.53728
Epoch# 9 of 10 epochs.
epoch_loss = (561.132), Average epoch_loss = 9.51072
Epoch# 10 of 10 epochs.
epoch_loss = (559.24), Average epoch_loss = 9.47864
Trained input weights written to file: w1p.dat
Trained output weights written to file: w2p.dat

.\weights.exe feeling cough throat tired w1 w1p.dat w2 w2p.dat
W1: 54 X 16
W2: 16 X 54
W2 transposed: 54 X 16
feeling -> cough
Cosine Similarity = 0.25121, Cosine Distance = 0.74879
feeling -> throat
Cosine Similarity = -0.557122, Cosine Distance = 0.442878
feeling -> tired
Cosine Similarity = 0.335205, Cosine Distance = 0.664795
cough -> throat
Cosine Similarity = -0.376468, Cosine Distance = 0.623532
cough -> tired
Cosine Similarity = 0.21912, Cosine Distance = 0.78088
throat -> tired
Cosine Similarity = -0.0624799, Cosine Distance = 0.93752
```

#### Analysis of Training and Cosine Similarities.

**_Training Observations_**.

1. _Model Dimensions_:
- Input weight matrix `W1:54×16`
(54 unique tokens in the vocabulary, 16-dimensional embeddings).
- Output weight matrix `W2:16*54`
(W1 transposed`(`in terms of just shape`)` `54x16` for similarity calculations).

2. _Loss Reduction_:
- The total loss starts at `574.778` in epoch `1` and decreases to `559.24` by epoch `10`.
- The steady decline in loss indicates that the model is learning effectively, though the loss reduction per epoch is modest.
- The small learning rate `0.0001` ensures stable convergence but may require more epochs for significant improvements.

3. _Average Epoch Loss_:
- Initial average loss per data point: `9.742`.
- Final average loss: `9.478`.
- The reduction is consistent, but the relatively high values suggest the embeddings may still require further fine-tuning or additional data.


**_Cosine Similarity Results_**.
1. _General Trends_: 
- The cosine similarity values for word pairs range from `-0.557` to `0.335`, indicating that the learned embeddings are not yet well-clustered semantically.
- Positive similarity values suggest some learned relationships, while negative values reflect weak or inverse relationships.

2. _Detailed Pair Analysis_:

- `feeling` -> `cough`: Cosine Similarity = `0.25121` (moderately low).  Indicates a weak but somewhat meaningful relationship. Both words appear in a health-related context, but their distance suggests limited overlap in usage in the training corpus.
- `feeling` -> `throat`: Cosine Similarity = `−0.557122`(negative).Suggests no meaningful semantic relationship between these two words in the current embeddings.
- `feeling` -> `tired`: Cosine Similarity = `0.335205`(highest among the pairs). A stronger relationship, reflecting that "feeling" and "tired" often co-occur in similar contexts in the corpus.
- `cough` -> `throat`: Cosine Similarity = `−0.376468`(negative). Despite a potential real-world relationship, the embeddings fail to capture it, possibly due to insufficient co-occurrences in the small corpus.
- `cough` -> `tired`: Cosine Similarity = `0.21912`(low). Indicates weak contextual similarity.
- `throat` -> `tired`: Cosine Similarity = `−0.0624799`(near zero). No significant relationship between these words in the embeddings.


**_Analysis of Context Window and Corpus Impact_**.
1. _Small Corpus_: The limited training corpus `(`54 tokens`)` provides very few co-occurrence patterns for the model to learn meaningful semantic relationships. Sparse data results in embeddings that are not fully optimized.
2. _Context Window_: With a context window of size `4`, relationships are only established within a short span of words. This limits the model's ability to capture long-range semantic relationships, particularly for words that rarely co-occur in close proximity.
3. _High Dimensionality_: The embedding dimension `(`16`)` may be too high relative to the size of the corpus. This could lead to overfitting and prevent the model from effectively generalizing.

**_Suggestions for Improvement_**.
1. _Increase Training Corpus Size_: Use a larger and more diverse dataset to provide more co-occurrence information, allowing the model to learn richer embeddings.
2. _Reduce Embedding Dimension_: Experiment with smaller embedding dimensions `(`e.g., 8 or 10`)` to better match the limited size of the corpus.
3. _Adjust Context Window_: Consider increasing the context window size `(`e.g., to 6 or 8`)` to capture broader relationships, especially if longer phrases or sentences are added to the corpus.
4. _Evaluate with Word Analogies_: Test embeddings using analogy tasks `(`e.g., "feeling" is to "tired" as "cough" is to ?`)` to better assess semantic clustering.
5. _Add Noise Contrastive Estimation (NCE)_: Implement negative sampling or `NCE` to focus learning on meaningful context-target pairs while reducing noise.

**_Conclusion_**.
The embeddings from the Skip-gram implementation show initial signs of learning but are constrained by the small training corpus and limited context. Fine-tuning hyperparameters and expanding the dataset will be crucial for improving semantic clustering and achieving meaningful relationships between words.

### Observations from Second Training Session.

```
.\skipy.exe corpus ./INPUT.txt lr 0.0001 epoch 10 rs 0.001 loop 0 verbose --input w1p.dat w2p.dat --output w1.dat w2.dat
Corpus: ./INPUT.txt
Dimensions of W1 = 54 X 16
Dimensions of W2 = 16 X 54
Epoch# 1 of 10 epochs.
epoch_loss = (557.422), Average epoch_loss = 9.44783
Epoch# 2 of 10 epochs.
epoch_loss = (555.677), Average epoch_loss = 9.41826
Epoch# 3 of 10 epochs.
epoch_loss = (554.077), Average epoch_loss = 9.39113
Epoch# 4 of 10 epochs.
epoch_loss = (552.848), Average epoch_loss = 9.3703
Epoch# 5 of 10 epochs.
epoch_loss = (551.056), Average epoch_loss = 9.33993
Epoch# 6 of 10 epochs.
epoch_loss = (549.238), Average epoch_loss = 9.30911
Epoch# 7 of 10 epochs.
epoch_loss = (547.468), Average epoch_loss = 9.27912
Epoch# 8 of 10 epochs.
epoch_loss = (545.743), Average epoch_loss = 9.24987
Epoch# 9 of 10 epochs.
epoch_loss = (544.026), Average epoch_loss = 9.22078
Epoch# 10 of 10 epochs.
epoch_loss = (542.629), Average epoch_loss = 9.1971
Trained input weights written to file: w1.dat
Trained output weights written to file: w2.dat

.\weights.exe feeling cough throat tired w1 w1.dat w2 w2.dat 
W1: 54 X 16
W2: 16 X 54
W2 transposed: 54 X 16
feeling -> cough
Cosine Similarity = 0.2518, Cosine Distance = 0.7482
feeling -> throat
Cosine Similarity = -0.55438, Cosine Distance = 0.44562
feeling -> tired
Cosine Similarity = 0.336756, Cosine Distance = 0.663244
cough -> throat
Cosine Similarity = -0.374292, Cosine Distance = 0.625708
cough -> tired
Cosine Similarity = 0.221063, Cosine Distance = 0.778937
throat -> tired
Cosine Similarity = -0.057959, Cosine Distance = 0.942041
```

**_Training Metrics_**.
1. _Loss Progression_:
    - The total loss continues to decline steadily, starting at `557.422` in epoch `1` and reaching `542.629` in epoch `10`
    - Average epoch loss shows consistent improvement: Initial `9.44783` Final `9.1971`.
    - The consistent drop in loss indicates that the model is learning, but the reduction rate remains modest. This could be due to a relatively small learning rate `(`0.0001`)` and limited corpus size.
2. _Initialization with Pretrained Weights_:
    - By starting with weights from the previous session `(`w1p.dat, w2p.dat`)`, the model builds upon prior knowledge rather than starting anew. This approach is effective in fine-tuning and optimizing embeddings further.
    
**_Cosine Similarity Comparisons_**.

| Word Pair         | Cosine Similarity(First Session) | Cosine Similarity(Second Session) | Change
|-------------------|----------------------------------|-----------------------------------|------------------------------|
| feeling -> cough  | 0.25121                          | 0.2518                            | Slight increase (+0.00059)
| feeling -> throat | −0.557122                        | −0.55438                          |Slight improvement (+0.00274)
| feeling -> tired  | 0.335205                         | 0.336756                          | Slight increase (+0.00155)
| cough -> throat   | −0.376468                        | −0.374292                         | Improvement (+0.002176)
| cough -> tired    | 0.21912                          | 0.221063                          | Increase (+0.001943)
| throat -> tired   | −0.0624799                       | −0.057959                         | Improvement (+0.0045209)

1. _Improvement Trends_:  
    - Most word pairs show marginal improvements in cosine similarity, reflecting the ongoing fine-tuning of embeddings.
    - Relationships like `"feeling → tired" `and "`cough → tired" a`re slightly stronger, suggesting the model is starting to cluster semantically related words more effectively.
2. _Persistent Weaknesses_:
    - Some word pairs, such as `"feeling -> throat"` and  `"cough -> throat"` still exhibit negative cosine similarities, indicating limited semantic alignment.
    - The corpus may lack sufficient co-occurrence data to reinforce these relationships, or the embeddings may require further training.

**_Key Insights_**;
1. _Learning Continuity_: Continuing training with pretrained weights has proven effective, as evidenced by the steady loss reduction and slight improvements in similarity scores.
2. _Data Limitation_: The small training corpus remains a bottleneck, limiting the richness of learned embeddings. Expanding the dataset would likely yield more substantial improvements.
3. _Subtle Gains_: The improvements in cosine similarity are minor, which is expected given the modest adjustments in weights during each epoch.

**_Suggestions for Next Steps_**.
1. _Extend Training_: Increase the number of epochs for this fine-tuning phase to observe if the loss continues to drop and embeddings improve further.
2. _Incorporate New Data_: Add more sentences to the corpus to improve co-occurrence statistics and help the model generalize better.
3. _Analyze Contextual Relationships_: Perform additional evaluations with more word pairs to determine if improvements are consistent across the vocabulary.
4. _Adjust Learning Rate_: Consider a slightly higher learning rate (0.001) for future sessions to accelerate convergence without overshooting.

By addressing these areas, the embeddings can evolve into a more meaningful representation of the corpus semantics.

### Observations from Thrid Training Session.

***`The pace of loss reduction has slowed slightly compared to earlier sessions, indicating the model may be approaching an optimal representation of the current corpus`***

```
.\skipy.exe corpus ./INPUT.txt lr 0.0001 epoch 10 rs 0.001 loop 0 verbose --input w1.dat w2.dat --output w1-1.dat w2-1.dat
Corpus: ./INPUT.txt
Dimensions of W1 = 54 X 16
Dimensions of W2 = 16 X 54
Epoch# 1 of 10 epochs.
epoch_loss = (540.924), Average epoch_loss = 9.16821
Epoch# 2 of 10 epochs.
epoch_loss = (539.021), Average epoch_loss = 9.13596
Epoch# 3 of 10 epochs.
epoch_loss = (537.401), Average epoch_loss = 9.1085
Epoch# 4 of 10 epochs.
epoch_loss = (535.916), Average epoch_loss = 9.08331
Epoch# 5 of 10 epochs.
epoch_loss = (534.298), Average epoch_loss = 9.05589
Epoch# 6 of 10 epochs.
epoch_loss = (532.723), Average epoch_loss = 9.02921
Epoch# 7 of 10 epochs.
epoch_loss = (531.341), Average epoch_loss = 9.00579
Epoch# 8 of 10 epochs.
epoch_loss = (529.663), Average epoch_loss = 8.97733
Epoch# 9 of 10 epochs.
epoch_loss = (528.252), Average epoch_loss = 8.95342
Epoch# 10 of 10 epochs.
epoch_loss = (527.001), Average epoch_loss = 8.93222
Trained input weights written to file: w1-1.dat
Trained output weights written to file: w2-1.dat

.\weights.exe feeling cough throat tired w1 w1-1.dat w2 w2-1.dat
W1: 54 X 16
W2: 16 X 54
W2 transposed: 54 X 16
feeling -> cough
Cosine Similarity = 0.252387, Cosine Distance = 0.747613
feeling -> throat
Cosine Similarity = -0.551453, Cosine Distance = 0.448547
feeling -> tired
Cosine Similarity = 0.338585, Cosine Distance = 0.661415
cough -> throat
Cosine Similarity = -0.37179, Cosine Distance = 0.62821
cough -> tired
Cosine Similarity = 0.223083, Cosine Distance = 0.776917
throat -> tired
Cosine Similarity = -0.053339, Cosine Distance = 0.946661
```

### Training Metrics.
1. ***_Loss Progression:_***:
    - _The loss continues its steady decline_: Starting loss at epoch: `540.924`, Ending loss at epoch 10: `527.001`.
    - _Average epoch loss decreased_: From `9.16821` to `8.93222`, demonstrating consistent improvement in model performance.
2. ***_Comparison Across Sessions_***: `The pace of loss reduction has slowed slightly compared to earlier sessions, indicating the model may be approaching an optimal representation of the current corpus`.

**_Cosine Similarity Comparisons_**.

| Word Pair         | Cosine Similarity(Second Session) | Cosine Similarity(Third Session) | Change
|-------------------|----------------------------------|-----------------------------------|------------------------------|
| feeling -> cough  | 0.2518                            | 0.252387                         | Slight increase (+0.000587)
| feeling -> throat | −0.55438                          | −0.551453                        | Improvement (+0.002927)
| feeling -> tired  | 0.336756                          | 0.338585                         | Increase (+0.001829)
| cough -> throat   | −0.374292                         |  −0.37179                        | Improvement (+0.002502)
| cough -> tired    | 0.221063                          | 0.223083                         | Increase (+0.00202)
| throat -> tired   | −0.057959                         | −0.053339                        | Improvement (+0.00462)

1. _Continued Improvements_:
    - Marginal but consistent increases in cosine similarity scores are seen across most word pairs.
    - Relationships like `"feeling → throat"` and `"throat → tired"` _show small but meaningful refinements_, indicating the embeddings are better capturing semantic relationships.

2.  _Negative Similarities_: `"feeling → throat"` and `"cough → throat"` still show negative cosine similarities, though _the values are improving_. This suggests that while _the model is learning_, the **corpus may lack sufficient co-occurrence data** to fully connect these terms.



